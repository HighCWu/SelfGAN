{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self_gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "_HroZbwijzUd",
        "gZd0PC8ddn6a"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qXRAKEQFaPy8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Self GAN\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\" >\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/HighCWu/SelfGAN/blob/master/implementations/gan/self_gan.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/HighCWu/SelfGAN/blob/master/implementations/gan/self_gan.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "cKiW6WoRcHGf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare"
      ]
    },
    {
      "metadata": {
        "id": "iv57iXGFL1iu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "os.makedirs('images', exist_ok=True)\n",
        "os.makedirs('images_normal', exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iust7u4wTi4a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
        "parser.add_argument('--batch_size', type=int, default=64, help='size of the batches')\n",
        "parser.add_argument('--lr', type=float, default=2e-4, help='adam: learning rate')\n",
        "parser.add_argument('--b1', type=float, default=0.5, help='adam: decay of first order momentum of gradient')\n",
        "parser.add_argument('--b2', type=float, default=0.999, help='adam: decay of first order momentum of gradient')\n",
        "parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
        "parser.add_argument('--latent_dim', type=int, default=100, help='dimensionality of the latent space')\n",
        "parser.add_argument('--img_size', type=int, default=28, help='size of each image dimension')\n",
        "parser.add_argument('--channels', type=int, default=1, help='number of image channels')\n",
        "parser.add_argument('--sample_interval', type=int, default=400, help='interval betwen image samples')\n",
        "\n",
        "opt,_ = parser.parse_known_args()\n",
        "print(opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ng6pkwk6Vq5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rLPXMaDWXeMV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity\n",
        "      \n",
        "class SelfGAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SelfGAN, self).__init__()\n",
        "\n",
        "        # Initialize generator and discriminator\n",
        "        self.generator = Generator()\n",
        "        self.discriminator = Discriminator()\n",
        "\n",
        "    def forward(self, z, real_img, fake_img):\n",
        "        gen_img = self.generator(z)\n",
        "        validity_gen = self.discriminator(gen_img)\n",
        "        validity_real = self.discriminator(real_img)\n",
        "        validity_fake = self.discriminator(fake_img)\n",
        "\n",
        "        return gen_img, validity_gen, validity_real, validity_fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iiRxa6X0df0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SelfGAN Part"
      ]
    },
    {
      "metadata": {
        "id": "jsGndq9kXx6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "shard_adversarial_loss = torch.nn.BCELoss(reduction='none')\n",
        "\n",
        "# Initialize SelfGAN model\n",
        "self_gan = SelfGAN()\n",
        "\n",
        "if cuda:\n",
        "    self_gan.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    shard_adversarial_loss.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "os.makedirs('data/mnist', exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data/mnist', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                   ])),\n",
        "    batch_size=opt.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Optimizers\n",
        "optimizer = torch.optim.Adam(self_gan.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "last_imgs = Tensor(opt.batch_size, *img_shape)*0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jWgX7rkFjqtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Standard performance on the GPU"
      ]
    },
    {
      "metadata": {
        "id": "jENYgMC5eDER",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train SelfGAN\n",
        "        # -----------------\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs, validity_gen, validity_real, validity_fake = self_gan(z, real_imgs, last_imgs)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator and measure discriminator's ability to classify real from generated samples at the same time\n",
        "        gen_loss = adversarial_loss(validity_gen, valid)\n",
        "        real_loss = adversarial_loss(validity_real, valid)\n",
        "        fake_loss = adversarial_loss(validity_fake, fake)\n",
        "        v_g = 1 - torch.mean(validity_gen)\n",
        "        v_f = torch.mean(validity_fake)\n",
        "        s_loss = (real_loss + v_g*gen_loss*0.1 + v_f*fake_loss*0.9) / 2\n",
        "\n",
        "        s_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        last_imgs = gen_imgs.detach()\n",
        "        \n",
        "        sys.stdout.flush()\n",
        "        print (\"\\r[Epoch %d/%d] [Batch %d/%d] [S loss: %f  R loss: %f  F loss: %f  G loss: %f]\" % (epoch, opt.n_epochs, i, len(dataloader),\n",
        "                                                            s_loss.item(), real_loss.item(), fake_loss.item(), gen_loss.item()),\n",
        "              end='')\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_HroZbwijzUd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running on the GPU with similar performance of running on the TPU (Maybe)"
      ]
    },
    {
      "metadata": {
        "id": "suZwxsRuh7N9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train SelfGAN\n",
        "        # -----------------\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
        "        \n",
        "        s = opt.batch_size//8\n",
        "        for k in range(8):\n",
        "          # Generate a batch of images\n",
        "          gen_imgs, validity_gen, validity_real, validity_fake = self_gan(z[k*s:k*s+s], real_imgs[k*s:k*s+s], last_imgs[k*s:k*s+s])\n",
        "\n",
        "          # Loss measures generator's ability to fool the discriminator and measure discriminator's ability to classify real from generated samples at the same time\n",
        "          gen_loss = shard_adversarial_loss(validity_gen, valid[k*s:k*s+s])\n",
        "          real_loss = shard_adversarial_loss(validity_real, valid[k*s:k*s+s])\n",
        "          fake_loss = shard_adversarial_loss(validity_fake, fake[k*s:k*s+s])\n",
        "          v_g = 1 - torch.mean(validity_gen)\n",
        "          v_r = 1 - torch.mean(validity_real)\n",
        "          v_f = torch.mean(validity_fake)\n",
        "          v_sum = v_g + v_r + v_f\n",
        "          s_loss = v_r*real_loss/v_sum + v_g*gen_loss/v_sum + v_f*fake_loss/v_sum\n",
        "          \n",
        "          gen_loss = torch.mean(gen_loss)\n",
        "          real_loss = torch.mean(real_loss)\n",
        "          fake_loss = torch.mean(fake_loss)\n",
        "          s_loss = torch.mean(s_loss)\n",
        "\n",
        "          s_loss.backward()\n",
        "          last_imgs[k*s:k*s+s] = gen_imgs.detach()\n",
        "          \n",
        "        optimizer.step()\n",
        "        \n",
        "        sys.stdout.flush()\n",
        "        print (\"\\r[Epoch %d/%d] [Batch %d/%d] [S loss: %f  R loss: %f  F loss: %f  G loss: %f]\" % (epoch, opt.n_epochs, i, len(dataloader),\n",
        "                                                            s_loss.item(), real_loss.item(), fake_loss.item(), gen_loss.item()),\n",
        "              end='')\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            save_image(last_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZd0PC8ddn6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normal GAN Part"
      ]
    },
    {
      "metadata": {
        "id": "dv8ovPfrdsaC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "os.makedirs('data/mnist', exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data/mnist', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                   ])),\n",
        "    batch_size=opt.batch_size, shuffle=True)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tlmNSQ2Odykq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        \n",
        "        sys.stdout.flush()\n",
        "        print (\"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, opt.n_epochs, i, len(dataloader),\n",
        "                                                            d_loss.item(), g_loss.item()), \n",
        "               end='')\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], 'images_normal/%d.png' % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}